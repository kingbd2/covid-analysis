{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import *\n",
    "sqlalchemy.__version__\n",
    "import csv\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date \n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDataset:\n",
    "    ### INITIALIZE ###\n",
    "    def __init__(self):\n",
    "        # Create dataset paths and get latest data\n",
    "        self.BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(os.path.abspath(''))))\n",
    "        print(self.BASE_DIR)\n",
    "        self.reference_data_path = os.path.join(self.BASE_DIR, 'covid/data/reference')\n",
    "        self.timeseries_combined_data_path = os.path.join(self.BASE_DIR, 'covid/data/timeseries/daily_combined')\n",
    "        self.timeseries_split_data_path = os.path.join(self.BASE_DIR, 'covid/data/timeseries/daily_split')\n",
    "        self.timeseries_combined_files = [f for f in listdir(self.timeseries_combined_data_path) if isfile(os.path.join(self.timeseries_combined_data_path, f))]\n",
    "        self.timeseries_split_files = [f for f in listdir(self.timeseries_split_data_path) if isfile(os.path.join(self.timeseries_split_data_path, f))]\n",
    "        latest_data_file = max(self.timeseries_combined_files)\n",
    "        self.latest_data_date = str(latest_data_file)[0:10]\n",
    "        \n",
    "        # Compile dictionary of data sources\n",
    "        self.sources = {\n",
    "            \"confirmed\": {\n",
    "                \"url\": \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\",\n",
    "                \"local_path\": str(self.timeseries_split_data_path) + '/' + str(self.latest_data_date) + 'confirmed.csv'\n",
    "            },\n",
    "            \"recovered\": {\n",
    "                \"url\": \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\",\n",
    "                \"local_path\": str(self.timeseries_split_data_path) + '/' + str(self.latest_data_date) + 'recovered.csv'    \n",
    "            },\n",
    "            \"deaths\": {\n",
    "                \"url\": \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\",\n",
    "                \"local_path\": str(self.timeseries_split_data_path) + '/' + str(self.latest_data_date) + 'deaths.csv'\n",
    "            },\n",
    "            \"combined\": {\n",
    "                \"local_path\": str(self.timeseries_combined_data_path) + '/' + str(self.latest_data_date) + '-combined.csv'\n",
    "            },\n",
    "        }\n",
    "        self.confirmed_data_raw = {\n",
    "            'new': self.getNewData('confirmed'),\n",
    "            'old': self.getOldData('confirmed')\n",
    "            }\n",
    "        self.recovered_data_raw = {\n",
    "            'new': self.getNewData('recovered'),\n",
    "            'old': self.getOldData('recovered')\n",
    "            }\n",
    "        self.deaths_data_raw = {\n",
    "            'new': self.getNewData('deaths'),\n",
    "            'old': self.getOldData('deaths')\n",
    "            }\n",
    "        self.full_dataset_raw = [self.confirmed_data_raw, self.recovered_data_raw, self.deaths_data_raw]\n",
    "\n",
    "        self.reference_data = self.loadReferenceData()\n",
    "        self.needs_reference_data_refresh = False\n",
    "        self.needs_timeseries_data_refresh = False\n",
    "    \n",
    "    ### CREATE DATASETS (REFERENCE AND TIMESERIES) ###\n",
    "    \n",
    "    def createNewReferenceData(self):\n",
    "        # Continents reference data\n",
    "        continents_df = []\n",
    "        continents = [\"Africa\", \"Antarctica\", \"Asia\", \"Europe\", \"North America\", \"Oceania\", \"South America\", \"None\"]\n",
    "        continents_df = pd.DataFrame(continents)\n",
    "        continent_ids = list(range(0, len(continents)))\n",
    "        continent_id_column_name = 'continent_id'\n",
    "        continents_df[continent_id_column_name] = continent_ids\n",
    "        continents_df.columns = ['name', 'continent_id']\n",
    "        # Countries-continents reference data\n",
    "        base_path = self.reference_data_path\n",
    "        countries_continents_path = base_path + \"/country_continent.csv\"\n",
    "        countries_continents_df=pd.read_csv(countries_continents_path, index_col=0)\n",
    "        # Types reference data\n",
    "        types_df = pd.DataFrame([\"Confirmed\", \"Recovered\", \"Deaths\"])\n",
    "        type_ids = list(range(0, len(types_df)))\n",
    "        type_id_column_name = 'type_id'\n",
    "        types_df[type_id_column_name] = type_ids\n",
    "        types_df.columns = ['name', 'type_id']\n",
    "        # Province/State reference data\n",
    "        province_state_df = self.deduplicate(\"Province/State\", by_column_range=True, up_to_column=4)\n",
    "        # Country reference data\n",
    "        country_df = self.deduplicate(\"Country/Region\", by_column_range=True, up_to_column=4, summarize=True, summary_column='Country/Region')\n",
    "        province_state_df_merged = pd.merge(province_state_df, country_df, how='inner', on='Country/Region', left_index=False, right_index=False, sort=False,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "        province_state_df_merged = province_state_df_merged.drop('Country/Region', axis=1).drop('Lat_y', axis=1).drop('Long_y', axis=1)\n",
    "        province_state_df_merged.columns = ['name', 'lat', 'long', 'province_state_id', 'country_id']\n",
    "        country_df_merged = pd.merge(country_df, countries_continents_df, how='inner', on='Country/Region', left_index=False, right_index=False, sort=False,\n",
    "         suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)\n",
    "        country_df_merged = country_df_merged.drop('Continent', axis=1)\n",
    "        country_df_merged.columns = ['name', 'lat', 'long', 'country_id', 'continent_id']\n",
    "        # Combine all reference data into a list\n",
    "        reference_data_list = [continents_df, countries_continents_df, country_df_merged, province_state_df_merged, types_df]\n",
    "        keys = ['continent', 'country_continent','country', 'province_state', 'types']\n",
    "        values = reference_data_list\n",
    "        reference_data_dict = dict(zip(keys, values))\n",
    "        self.new_reference_data = reference_data_dict\n",
    "        return reference_data_dict\n",
    "    \n",
    "    def loadReferenceData(self):\n",
    "        base_path = self.reference_data_path\n",
    "        continent_path = base_path + \"/continent.csv\"\n",
    "        country_path = base_path + \"/country.csv\"\n",
    "        province_state_path = base_path + \"/province_state.csv\"\n",
    "        countries_continents_path = base_path + \"/country_continent.csv\"\n",
    "        types_path = base_path + \"/types.csv\"\n",
    "        reference_data_paths = [continent_path, countries_continents_path, country_path, province_state_path, types_path]\n",
    "        reference_data_list = []\n",
    "        for path in reference_data_paths:\n",
    "            df = pd.read_csv(path, index_col=0)\n",
    "            reference_data_list.append(df)\n",
    "        keys = ['continent', 'country_continent','country', 'province_state', 'types']\n",
    "        values = reference_data_list\n",
    "        reference_data_dict = dict(zip(keys, values))\n",
    "        return reference_data_dict\n",
    "    \n",
    "    ### ACTIONS ###\n",
    "    def getNewData(self, data_type):\n",
    "        \"\"\"Input url of csv, returns dataframe\"\"\"\n",
    "        df = pd.read_csv(self.sources[str(data_type)]['url'])\n",
    "        return df\n",
    "\n",
    "    def getOldData(self, data_type):\n",
    "        \"\"\"Input filepath of csv, returns dataframe\"\"\"\n",
    "        df = pd.read_csv(self.sources[str(data_type)]['local_path'])\n",
    "        return df\n",
    "    \n",
    "    def deduplicate(self, column_name: str, by_column_range=False, up_to_column=4, summarize=False, summary_column='test'):\n",
    "        item_list = []\n",
    "        if by_column_range==True and summarize==False:\n",
    "            for dataset in self.full_dataset_raw:\n",
    "                items = pd.DataFrame(dataset['new'].iloc[:,0:up_to_column].drop_duplicates(keep='first').dropna().reset_index().drop([\"index\"],axis=1))\n",
    "                item_list.append(items)\n",
    "        elif by_column_range==True and summarize==True:\n",
    "            for dataset in self.full_dataset_raw:\n",
    "                data_by_column = pd.DataFrame(dataset['new'].iloc[:,0:up_to_column]).groupby(summary_column, as_index=False).mean()\n",
    "                items = data_by_column.iloc[:,0:up_to_column].drop_duplicates(subset=summary_column, keep='first').dropna().reset_index()\n",
    "                item_list.append(data_by_column)\n",
    "        else:\n",
    "            for dataset in self.full_dataset_raw:\n",
    "                items = pd.DataFrame(dataset['new'][[column_name]].drop_duplicates(keep='first').dropna().reset_index().drop([\"index\"],axis=1))\n",
    "                item_list.append(items)\n",
    "        concatenated_df = pd.concat([item_list[0], item_list[1], item_list[2]], ignore_index=True)\n",
    "        item_df = concatenated_df.drop_duplicates(subset=column_name, keep='first')\n",
    "        item_ids = list(range(0, len(item_df)))\n",
    "        id_column_name = column_name + '_id'\n",
    "        item_df[id_column_name] = item_ids\n",
    "        return item_df\n",
    "    \n",
    "    def saveData(self):\n",
    "        timeseries_filenames = [\"confirmed\", \"recovered\", \"deaths\"]\n",
    "        reference_filenames = ['continent', 'country_continent', 'country', 'province_state', 'types']\n",
    "        base_dir_split = self.timeseries_split_data_path\n",
    "        base_dir_combined = self.timeseries_combined_data_path\n",
    "        base_dir_reference = self.reference_data_path\n",
    "        for i, item in enumerate(self.full_dataset_cleaned_list):\n",
    "            filename = base_dir_split + '/' + str(date.today()) + str(timeseries_filenames[i]) + \".csv\" \n",
    "            item.to_csv(filename)\n",
    "        filename_combined = base_dir_combined + '/' + str(date.today()) + \"-combined.csv\" \n",
    "        self.full_dataset_cleaned_combined.to_csv(filename_combined)\n",
    "        \n",
    "        for i, item in enumerate(self.new_reference_data.items()):\n",
    "            filename = base_dir_reference + '/' + str(reference_filenames[i]) + \".csv\"\n",
    "            if reference_filenames[i]=='country_continent':\n",
    "                continue\n",
    "            item[1].to_csv(filename, index=False)\n",
    "            \n",
    "    def standardizeNewData(self):\n",
    "        full_dataset_cleaned = []\n",
    "        for i, dataset in enumerate(self.full_dataset_raw):\n",
    "            # Join country_id, prov_state_id\n",
    "            dataset_merged_country = pd.merge(dataset['new'], self.new_reference_data['country'].iloc[:,[0,3]], how='inner', on=None, left_on='Country/Region', right_on='name', sort=False,\n",
    "                               suffixes=('_x', '_y'), copy=False, indicator=False, validate=None)\n",
    "            dataset_merged_province = pd.merge(dataset_merged_country, self.new_reference_data['province_state'].iloc[:,[0,3]], how='left', on=None, left_on='Province/State', right_on='name', sort=False,\n",
    "                               suffixes=('_x', '_y'), copy=False, indicator=False, validate=None)\n",
    "            # Drop Lat, Long, Province/State, Country/Region\n",
    "            cases = dataset_merged_province.drop('Country/Region', axis=1).drop(\"name_x\", axis=1).drop(\"name_y\", axis=1).drop(\"Lat\", axis=1).drop(\"Long\", axis=1).drop(\"Province/State\", axis=1)\n",
    "            # Melt dataframe using date columns as rows\n",
    "            cases_melt = pd.melt(cases, id_vars=['country_id', 'province_state_id'], value_vars=['1/22/20'])\n",
    "            # Join all melted time series columns\n",
    "            for j, column in enumerate(cases.iloc[:,1:-2]):\n",
    "                melted_df = pd.melt(cases, id_vars=['country_id', 'province_state_id'], value_vars=[column])\n",
    "                cases_melt = cases_melt.append(melted_df)\n",
    "            cases_melt.columns = ['country_id', 'province_state_id', 'date', 'count']\n",
    "            cases_melt['date'] = pd.to_datetime(cases_melt['date'],infer_datetime_format=True)\n",
    "            cases_melt[\"country_id\"] = pd.to_numeric(cases_melt[\"country_id\"], downcast='integer')\n",
    "            cases_melt[\"province_state_id\"] = pd.to_numeric(cases_melt[\"province_state_id\"], downcast='integer')\n",
    "            cases_melt[\"count\"] = pd.to_numeric(cases_melt[\"count\"], downcast='integer')\n",
    "            cases_melt['case_type'] = i\n",
    "            full_dataset_cleaned.append(cases_melt)\n",
    "        self.full_dataset_cleaned_list = full_dataset_cleaned\n",
    "        full_dataset_cleaned_combined = pd.concat(full_dataset_cleaned, ignore_index=True)\n",
    "        full_dataset_cleaned_combined['province_state_id'] = full_dataset_cleaned_combined['province_state_id'].astype('Int64')\n",
    "        full_dataset_cleaned_combined['country_id'] = full_dataset_cleaned_combined['country_id'].astype('Int64')\n",
    "        full_dataset_cleaned_combined['count'] = full_dataset_cleaned_combined['count'].astype('Int64')\n",
    "        self.full_dataset_cleaned_combined = full_dataset_cleaned_combined\n",
    "\n",
    "    def validateData(self, data_type):\n",
    "        # Validate reference data\n",
    "\n",
    "        # Compare columns\n",
    "        location_comparison = df.columns[0:4]==pd.Index(['Province/State', 'Country/Region', 'Lat', 'Long'])\n",
    "        if not location_comparison.all():\n",
    "            print(\"Location columns have changed\")\n",
    "\n",
    "        timeseries_comparison = df.columns[4:]==pd.Index(['Province/State', 'Country/Region', 'Lat', 'Long'])\n",
    "        if not location_comparison.all():\n",
    "            print(\"Location columns have changed\")\n",
    "\n",
    "        # Compare dataframe sizes\n",
    "        old_size = old_df.size    \n",
    "        new_size = new_df.size\n",
    "        if old_size!=new_size:\n",
    "            print(\"Data size mismatch: Old: \",  str(old_size), \"; New: \", str(new_size))\n",
    "        \n",
    "        # Compate dataframe content\n",
    "        df_diff = pd.concat([old_df,new_df]).drop_duplicates(keep=False)\n",
    "        if df_diff.empty:\n",
    "            print(\"No changes in data detected\")\n",
    "\n",
    "class CovidDatabase:\n",
    "    ### Initialize ###\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            COVID_DB_ENGINE_CONNECTION = os.getenv('COVID_DB_ENGINE_CONNECTION')\n",
    "            self.engine = create_engine(COVID_DB_ENGINE_CONNECTION, echo=True)\n",
    "            self.connection = self.engine.connect()\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Could not find database connection environment variable. Please create it using \"export COVID_DB_ENGINE_CONNECTION=\"postgresql+psycopg2://covid_superuser:PASSWORD@localhost:5432/covid\".')\n",
    "        metadata = MetaData()\n",
    "        # Continent table\n",
    "        self.metadata = {\n",
    "            'continent': Table('continent', metadata,\n",
    "                Column('continent_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "            ),\n",
    "            # Country table\n",
    "            'country': Table('country', metadata,\n",
    "                Column('country_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('continent_id', Integer, ForeignKey(\"continent.continent_id\"))\n",
    "            ),\n",
    "            # Province-state table\n",
    "            'province_state': Table('province_state', metadata,\n",
    "                Column('province_state_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('country_id', Integer, ForeignKey(\"country.country_id\"))\n",
    "            ),\n",
    "            # Type category table\n",
    "            'type_category': Table('type_category', metadata,\n",
    "                Column('type_category_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "            ),\n",
    "            # Case table\n",
    "            'case_timeseries': Table('case_timeseries', metadata,\n",
    "                Column('case_timeseries_id', Integer, primary_key=True),\n",
    "                Column('count', Numeric, nullable=True),\n",
    "                Column('date', TIMESTAMP, nullable=True),\n",
    "                Column('case_type', Integer, ForeignKey(\"type_category.type_category_id\")),\n",
    "                Column('country_id', Integer, ForeignKey(\"country.country_id\")),\n",
    "                Column('province_state_id', Integer, ForeignKey(\"province_state.province_state_id\"))\n",
    "            )\n",
    "        }\n",
    "    def getTable(self, table_name):\n",
    "        self.connection = self.engine.connect()\n",
    "        row_list=[]\n",
    "        with self.connection as con:\n",
    "            rs = con.execute(\"SELECT * FROM \" + str(table_name))\n",
    "            for row in rs:\n",
    "                row_list.append(row)\n",
    "\n",
    "        row_df = pd.DataFrame(row_list)\n",
    "        if table_name=='province_state':\n",
    "            row_df.columns = ['province_state_id', 'name', 'lat', 'long', 'country_id']\n",
    "        elif table_name=='country':\n",
    "            row_df.columns = ['country_id', 'name', 'lat', 'long', 'continent_id']\n",
    "        elif table_name=='continent':\n",
    "            row_df.columns = ['continent_id', 'name']\n",
    "        elif table_name=='type_category':\n",
    "            row_df.columns = ['type_id', 'name']\n",
    "        else:\n",
    "            row_df.columns = ['case_timeseries_id','case_type', 'count', 'country_id', 'date', 'province_state_id']\n",
    "        return(row_df)\n",
    "    def updateData(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatabase:\n",
    "    ### Initialize ###\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            COVID_DB_ENGINE_CONNECTION = os.getenv('COVID_DB_ENGINE_CONNECTION')\n",
    "            self.engine = create_engine(COVID_DB_ENGINE_CONNECTION, echo=True)\n",
    "            self.connection = self.engine.connect()\n",
    "        except AttributeError:\n",
    "            raise AttributeError('Could not find database connection environment variable. Please create it using \"export COVID_DB_ENGINE_CONNECTION=\"postgresql+psycopg2://covid_superuser:PASSWORD@localhost:5432/covid\".')\n",
    "        metadata = MetaData()\n",
    "        # Continent table\n",
    "        self.metadata = {\n",
    "            'continent': Table('continent', metadata,\n",
    "                Column('continent_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "            ),\n",
    "            # Country table\n",
    "            'country': Table('country', metadata,\n",
    "                Column('country_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('continent_id', Integer, ForeignKey(\"continent.continent_id\"))\n",
    "            ),\n",
    "            # Province-state table\n",
    "            'province_state': Table('province_state', metadata,\n",
    "                Column('province_state_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('lat', Numeric, nullable=True),\n",
    "                Column('country_id', Integer, ForeignKey(\"country.country_id\"))\n",
    "            ),\n",
    "            # Type category table\n",
    "            'type_category': Table('type_category', metadata,\n",
    "                Column('type_category_id', Integer, primary_key=True),\n",
    "                Column('name', String(50), nullable=True),\n",
    "            ),\n",
    "            # Case table\n",
    "            'case_timeseries': Table('case_timeseries', metadata,\n",
    "                Column('case_timeseries_id', Integer, primary_key=True),\n",
    "                Column('count', Numeric, nullable=True),\n",
    "                Column('date', TIMESTAMP, nullable=True),\n",
    "                Column('case_type', Integer, ForeignKey(\"type_category.type_category_id\")),\n",
    "                Column('country_id', Integer, ForeignKey(\"country.country_id\")),\n",
    "                Column('province_state_id', Integer, ForeignKey(\"province_state.province_state_id\"))\n",
    "            )\n",
    "        }\n",
    "    def getTable(self, table_name):\n",
    "        self.connection = self.engine.connect()\n",
    "        row_list=[]\n",
    "        with self.connection as con:\n",
    "            rs = con.execute(\"SELECT * FROM \" + str(table_name))\n",
    "            for row in rs:\n",
    "                row_list.append(row)\n",
    "\n",
    "        row_df = pd.DataFrame(row_list)\n",
    "        if table_name=='province_state':\n",
    "            row_df.columns = ['province_state_id', 'name', 'lat', 'long', 'country_id']\n",
    "        elif table_name=='country':\n",
    "            row_df.columns = ['country_id', 'name', 'lat', 'long', 'continent_id']\n",
    "        elif table_name=='continent':\n",
    "            row_df.columns = ['continent_id', 'name']\n",
    "        elif table_name=='type_category':\n",
    "            row_df.columns = ['type_id', 'name']\n",
    "        else:\n",
    "            row_df.columns = ['case_timeseries_id','case_type', 'count', 'country_id', 'count_date', 'province_state_id']\n",
    "        return(row_df)\n",
    "    def updateData(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bking/Projects/poetry/covid\n",
      "2020-04-29 16:31:41,185 INFO sqlalchemy.engine.base.Engine select version()\n",
      "2020-04-29 16:31:41,186 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-04-29 16:31:41,187 INFO sqlalchemy.engine.base.Engine select current_schema()\n",
      "2020-04-29 16:31:41,188 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-04-29 16:31:41,189 INFO sqlalchemy.engine.base.Engine SELECT CAST('test plain returns' AS VARCHAR(60)) AS anon_1\n",
      "2020-04-29 16:31:41,190 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-04-29 16:31:41,191 INFO sqlalchemy.engine.base.Engine SELECT CAST('test unicode returns' AS VARCHAR(60)) AS anon_1\n",
      "2020-04-29 16:31:41,191 INFO sqlalchemy.engine.base.Engine {}\n",
      "2020-04-29 16:31:41,192 INFO sqlalchemy.engine.base.Engine show standard_conforming_strings\n",
      "2020-04-29 16:31:41,193 INFO sqlalchemy.engine.base.Engine {}\n"
     ]
    }
   ],
   "source": [
    "current_data = CovidDataset()\n",
    "db_data = CovidDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid data keys: dict_keys(['BASE_DIR', 'reference_data_path', 'timeseries_combined_data_path', 'timeseries_split_data_path', 'timeseries_combined_files', 'timeseries_split_files', 'latest_data_date', 'sources', 'confirmed_data_raw', 'recovered_data_raw', 'deaths_data_raw', 'full_dataset_raw', 'reference_data', 'needs_reference_data_refresh', 'needs_timeseries_data_refresh'])\n",
      "Covid database keys: dict_keys(['engine', 'connection', 'metadata'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Covid data keys:\", current_data.__dict__.keys())\n",
    "print(\"Covid database keys:\", db_data.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BASE_DIR',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'confirmed_data_raw',\n",
       " 'createNewReferenceData',\n",
       " 'deaths_data_raw',\n",
       " 'deduplicate',\n",
       " 'full_dataset_raw',\n",
       " 'getNewData',\n",
       " 'getOldData',\n",
       " 'latest_data_date',\n",
       " 'loadReferenceData',\n",
       " 'needs_reference_data_refresh',\n",
       " 'needs_timeseries_data_refresh',\n",
       " 'recovered_data_raw',\n",
       " 'reference_data',\n",
       " 'reference_data_path',\n",
       " 'saveData',\n",
       " 'sources',\n",
       " 'standardizeNewData',\n",
       " 'timeseries_combined_data_path',\n",
       " 'timeseries_combined_files',\n",
       " 'timeseries_split_data_path',\n",
       " 'timeseries_split_files',\n",
       " 'validateData']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>province_state_id</th>\n",
       "      <th>country_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Australian Capital Territory</th>\n",
       "      <td>-35.4735</td>\n",
       "      <td>149.0124</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>New South Wales</th>\n",
       "      <td>-33.8688</td>\n",
       "      <td>151.2093</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Northern Territory</th>\n",
       "      <td>-12.4634</td>\n",
       "      <td>130.8456</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queensland</th>\n",
       "      <td>-28.0167</td>\n",
       "      <td>153.4000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Australia</th>\n",
       "      <td>-34.9285</td>\n",
       "      <td>138.6007</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montserrat</th>\n",
       "      <td>16.7425</td>\n",
       "      <td>-62.1874</td>\n",
       "      <td>71</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anguilla</th>\n",
       "      <td>18.2206</td>\n",
       "      <td>-63.0686</td>\n",
       "      <td>76</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>British Virgin Islands</th>\n",
       "      <td>18.4207</td>\n",
       "      <td>-64.6400</td>\n",
       "      <td>77</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turks and Caicos Islands</th>\n",
       "      <td>21.6940</td>\n",
       "      <td>-71.7979</td>\n",
       "      <td>78</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Falkland Islands (Malvinas)</th>\n",
       "      <td>-51.7963</td>\n",
       "      <td>-59.5236</td>\n",
       "      <td>80</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  lat      long  province_state_id  country_id\n",
       "name                                                                          \n",
       "Australian Capital Territory -35.4735  149.0124                  0           8\n",
       "New South Wales              -33.8688  151.2093                  1           8\n",
       "Northern Territory           -12.4634  130.8456                  2           8\n",
       "Queensland                   -28.0167  153.4000                  3           8\n",
       "South Australia              -34.9285  138.6007                  4           8\n",
       "...                               ...       ...                ...         ...\n",
       "Montserrat                    16.7425  -62.1874                 71         175\n",
       "Anguilla                      18.2206  -63.0686                 76         175\n",
       "British Virgin Islands        18.4207  -64.6400                 77         175\n",
       "Turks and Caicos Islands      21.6940  -71.7979                 78         175\n",
       "Falkland Islands (Malvinas)  -51.7963  -59.5236                 80         175\n",
       "\n",
       "[82 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data.reference_data['province_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-29 16:33:39,894 INFO sqlalchemy.engine.base.Engine SELECT * FROM province_state\n",
      "2020-04-29 16:33:39,895 INFO sqlalchemy.engine.base.Engine {}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>province_state_id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>country_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Australian Capital Territory                  ...</td>\n",
       "      <td>-35.4735</td>\n",
       "      <td>149.0124</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>New South Wales                               ...</td>\n",
       "      <td>-33.8688</td>\n",
       "      <td>151.2093</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Northern Territory                            ...</td>\n",
       "      <td>-12.4634</td>\n",
       "      <td>130.8456</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Queensland                                    ...</td>\n",
       "      <td>-28.0167</td>\n",
       "      <td>153.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>South Australia                               ...</td>\n",
       "      <td>-34.9285</td>\n",
       "      <td>138.6007</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>71</td>\n",
       "      <td>Montserrat                                    ...</td>\n",
       "      <td>16.7425</td>\n",
       "      <td>-62.1874</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>76</td>\n",
       "      <td>Anguilla                                      ...</td>\n",
       "      <td>18.2206</td>\n",
       "      <td>-63.0686</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>77</td>\n",
       "      <td>British Virgin Islands                        ...</td>\n",
       "      <td>18.4207</td>\n",
       "      <td>-64.64</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>78</td>\n",
       "      <td>Turks and Caicos Islands                      ...</td>\n",
       "      <td>21.69400000000001</td>\n",
       "      <td>-71.7979</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>80</td>\n",
       "      <td>Falkland Islands (Islas Malvinas)             ...</td>\n",
       "      <td>-51.7963</td>\n",
       "      <td>-59.5236</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    province_state_id                                               name  \\\n",
       "0                   0  Australian Capital Territory                  ...   \n",
       "1                   1  New South Wales                               ...   \n",
       "2                   2  Northern Territory                            ...   \n",
       "3                   3  Queensland                                    ...   \n",
       "4                   4  South Australia                               ...   \n",
       "..                ...                                                ...   \n",
       "77                 71  Montserrat                                    ...   \n",
       "78                 76  Anguilla                                      ...   \n",
       "79                 77  British Virgin Islands                        ...   \n",
       "80                 78  Turks and Caicos Islands                      ...   \n",
       "81                 80  Falkland Islands (Islas Malvinas)             ...   \n",
       "\n",
       "                  lat      long  country_id  \n",
       "0            -35.4735  149.0124           8  \n",
       "1            -33.8688  151.2093           8  \n",
       "2            -12.4634  130.8456           8  \n",
       "3            -28.0167     153.4           8  \n",
       "4            -34.9285  138.6007           8  \n",
       "..                ...       ...         ...  \n",
       "77            16.7425  -62.1874         175  \n",
       "78            18.2206  -63.0686         175  \n",
       "79            18.4207    -64.64         175  \n",
       "80  21.69400000000001  -71.7979         175  \n",
       "81           -51.7963  -59.5236         175  \n",
       "\n",
       "[82 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_data.getTable('province_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateData():\n",
    "    current_data = CovidDataset()\n",
    "    db_data = CovidDatabase()\n",
    "    \n",
    "    # Get current reference data\n",
    "    \n",
    "    \n",
    "    # Get current timeseries data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
